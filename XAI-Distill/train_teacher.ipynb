{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages and limit GPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'    # set visible devices depending on system configuration\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproducibilitySeed():\n",
    "    \"\"\"\n",
    "    Ensure reproducibility of results; Seeds to 0\n",
    "    \"\"\"\n",
    "    torch_init_seed = 0\n",
    "    torch.manual_seed(torch_init_seed)\n",
    "    numpy_init_seed = 0\n",
    "    np.random.seed(numpy_init_seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "reproducibilitySeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_image_shape = (28, 28)\n",
    "random_pad_size = 2\n",
    "\n",
    "# Training images augmented by randomly shifting images by at max. 2 pixels in any of 4 directions\n",
    "transform_train = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomCrop(mnist_image_shape, random_pad_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5], [0.5])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_val_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=True, \n",
    "                                            download=True, transform=transform_train)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./MNIST_dataset/', train=False, \n",
    "                                            download=True, transform=transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(1.0 * len(train_val_dataset) * 95 / 100)\n",
    "num_val = len(train_val_dataset) - num_train\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [num_train, num_val])\n",
    "\n",
    "train_val_loader = torch.utils.data.DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train teacher network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = 'checkpoints_teacher/'\n",
    "if not os.path.exists(checkpoints_path):\n",
    "    os.mkdir(checkpoints_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rates = list(np.logspace(-4, -2, 3))\n",
    "#learning_rates = [1e-2]\n",
    "learning_rate_decays = [0.95]    # learning rate decays at every epoch\n",
    "#weight_decays = [0.0] + list(np.logspace(-5, -1, 5))\n",
    "weight_decays = [1e-5]           # regularization weight\n",
    "momentums = [0.9]\n",
    "# dropout_probabilities = [(0.2, 0.5), (0.0, 0.0)]\n",
    "dropout_probabilities = [(0.0, 0.0),(0.2,0.3),(0.3,0.2),(0.5,0.5)]\n",
    "\n",
    "hparams_list = []\n",
    "for hparam_tuple in itertools.product(dropout_probabilities, weight_decays, learning_rate_decays, \n",
    "                                        momentums, learning_rates):\n",
    "    hparam = {}\n",
    "    hparam['dropout_input'] = hparam_tuple[0][0]\n",
    "    hparam['dropout_hidden'] = hparam_tuple[0][1]\n",
    "    hparam['weight_decay'] = hparam_tuple[1]\n",
    "    hparam['lr_decay'] = hparam_tuple[2]\n",
    "    hparam['lr'] = hparam_tuple[4]\n",
    "    hparams_list.append(hparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for hparam in hparams_list:\n",
    "    print('Training with hparams ' + utils.ToString(hparam))\n",
    "    reproducibilitySeed()\n",
    "    hparam_tuple = utils.DictToTuple(hparam)\n",
    "    results[hparam_tuple] = utils.trainTeacher(hparam, num_epochs, \n",
    "                                                train_loader, val_loader,\n",
    "                                                device=device)\n",
    "    \n",
    "    save_path = checkpoints_path + utils.ToString(hparam) + '_final.tar'\n",
    "    torch.save({'results' : results[hparam_tuple][0], \n",
    "                'model_state_dict' : results[hparam_tuple][1], \n",
    "                'epoch' : num_epochs}, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "utils.TeacherIterate('checkpoints_teacher', test_loader, device, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose dropout_hidden = 0.2, dropout_input=0.3, lr = 0.001, lr_decay=0.95, weight_decay = 1e-05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_ptorch",
   "language": "python",
   "name": "gpu_ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
