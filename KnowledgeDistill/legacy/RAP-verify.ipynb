{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "#from glob import glob\n",
    "import torch.backends.cudnn as cudnn\n",
    "from modules.resnet import resnet50\n",
    "import matplotlib.cm\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Original\n"
     ]
    }
   ],
   "source": [
    "arch = 'resnet50'\n",
    "# load the pre-trained weights\n",
    "model_file = 'ptrained-models/%s_places365.pth.tar' % arch\n",
    "model = models.__dict__[arch](num_classes=365)\n",
    "checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
    "state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print('Loaded Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded RAP Enabled Model\n"
     ]
    }
   ],
   "source": [
    "Rmodel = resnet50(num_classes=365)\n",
    "Rmodel.load_state_dict(state_dict)\n",
    "Rmodel.eval()\n",
    "print(\"Loaded RAP Enabled Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image transformer\n",
    "centre_crop = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load the class label\n",
    "file_name = 'categories_places365.txt'\n",
    "classes = list()\n",
    "with open(file_name) as class_file:\n",
    "    for line in class_file:\n",
    "        classes.append(line.strip().split(' ')[0][3:])\n",
    "classes = tuple(classes)\n",
    "\n",
    "# load the test image\n",
    "img_name = 'places365_standard/train/canal-urban/00000559.jpg'\n",
    "img = Image.open(img_name)\n",
    "input_img = V(centre_crop(img).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet50 original model prediction on places365_standard/train/canal-urban/00000559.jpg\n",
      "0.858 -> canal/urban\n",
      "0.045 -> tower\n",
      "0.027 -> bridge\n",
      "0.011 -> church/outdoor\n",
      "0.006 -> river\n",
      "resnet50 rap model prediction on places365_standard/train/canal-urban/00000559.jpg\n",
      "0.858 -> canal/urban\n",
      "0.045 -> tower\n",
      "0.027 -> bridge\n",
      "0.011 -> church/outdoor\n",
      "0.006 -> river\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "logit = model.forward(input_img)\n",
    "h_x = F.softmax(logit, 1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "\n",
    "print('{} original model prediction on {}'.format(arch,img_name))\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "\n",
    "# forward pass\n",
    "logit = Rmodel.forward(input_img)\n",
    "h_x = F.softmax(logit, 1).data.squeeze()\n",
    "probs, idx = h_x.sort(0, True)\n",
    "\n",
    "print('{} rap model prediction on {}'.format(arch,img_name))\n",
    "# output the prediction\n",
    "for i in range(0, 5):\n",
    "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enlarge_image(img, scaling = 3):\n",
    "    if scaling < 1 or not isinstance(scaling,int):\n",
    "        print ('scaling factor needs to be an int >= 1')\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        H,W = img.shape\n",
    "        out = np.zeros((scaling*H, scaling*W))\n",
    "        for h in range(H):\n",
    "            fh = scaling*h\n",
    "            for w in range(W):\n",
    "                fw = scaling*w\n",
    "                out[fh:fh+scaling, fw:fw+scaling] = img[h,w]\n",
    "    elif len(img.shape) == 3:\n",
    "        H,W,D = img.shape\n",
    "        out = np.zeros((scaling*H, scaling*W,D))\n",
    "        for h in range(H):\n",
    "            fh = scaling*h\n",
    "            for w in range(W):\n",
    "                fw = scaling*w\n",
    "                out[fh:fh+scaling, fw:fw+scaling,:] = img[h,w,:]\n",
    "    return out\n",
    "\n",
    "def hm_to_rgb(R, scaling = 3, cmap = 'bwr', normalize = True):\n",
    "    cmap = eval('matplotlib.cm.{}'.format(cmap))\n",
    "    if normalize:\n",
    "        R = R / np.max(np.abs(R)) # normalize to [-1,1] wrt to max relevance magnitude\n",
    "        R = (R + 1.)/2. # shift/normalize to [0,1] for color mapping\n",
    "    R = R\n",
    "    R = enlarge_image(R, scaling)\n",
    "    rgb = cmap(R.flatten())[...,0:3].reshape([R.shape[0],R.shape[1],3])\n",
    "    return rgb\n",
    "\n",
    "def visualize(relevances, img_name):\n",
    "    # visualize the relevance\n",
    "    n = len(relevances)\n",
    "    #print(n)\n",
    "    heatmap = np.sum(relevances.reshape([n, 224, 224, 1]), axis=3)\n",
    "    #print(heatmap.shape)\n",
    "    heatmaps = []\n",
    "    for h, heat in enumerate(heatmap):\n",
    "        #print(h,heat.shape)\n",
    "        maps = hm_to_rgb(heat, scaling=1, cmap = 'seismic')\n",
    "        heatmaps.append(maps)\n",
    "        im = Image.fromarray((maps*255).astype(np.uint8))\n",
    "        im.save('output_heatmap_'+img_name+'.png')\n",
    "        \n",
    "def visualize_v2(relevances, img_name):\n",
    "    # visualize the relevance\n",
    "    n = len(relevances)\n",
    "    #print(n)\n",
    "    heatmap = np.sum(relevances.reshape([n, 224, 224, 1]), axis=3)\n",
    "    #print(heatmap.shape)\n",
    "    heatmaps = []\n",
    "    for h, heat in enumerate(heatmap):\n",
    "        #print(h,heat.shape)\n",
    "        #maps = hm_to_rgb(heat, scaling=1, cmap = 'seismic')\n",
    "        #heatmaps.append(maps)\n",
    "        heat = heat / np.max(np.abs(heat))\n",
    "        heat = (heat + 1.)/2\n",
    "        im = Image.fromarray((heat*255).astype(np.uint8))\n",
    "        im.save('output_heatmap_'+img_name+'.png')\n",
    "        \n",
    "        \n",
    "\n",
    "def compute_pred(output):\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    print('Pred cls : '+str(pred))\n",
    "    T = pred.squeeze().cpu().numpy()\n",
    "    T = np.expand_dims(T, 0)\n",
    "    T = (T[:, np.newaxis] == np.arange(365)) * 1.0\n",
    "    T = torch.from_numpy(T).type(torch.FloatTensor)\n",
    "    Tt = V(T)\n",
    "    return Tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs4/ushashi/anaconda3/envs/gpu_ptorch/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred cls : tensor([[79]])\n",
      "(1, 224, 224, 1)\n",
      "(1, 224, 224, 1)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "input = V(input_img, volatile=True)\n",
    "input.requires_grad = True\n",
    "output = model(input)\n",
    "T = compute_pred(output)\n",
    "Res = Rmodel.relprop(R = output * T, alpha= 1).sum(dim=1, keepdim=True)\n",
    "heatmap = Res.permute(0, 2, 3, 1).data.cpu().numpy()\n",
    "print(heatmap.shape)\n",
    "visualize_v2(heatmap.reshape([1, 224, 224, 1]), 'lrp')\n",
    "\n",
    "\n",
    "RAP = Rmodel.RAP_relprop(R=T)\n",
    "Res = (RAP).sum(dim=1, keepdim=True)\n",
    "heatmap = Res.permute(0, 2, 3, 1).data.cpu().numpy()\n",
    "print(heatmap.shape)\n",
    "visualize(heatmap.reshape([1, 224, 224, 1]), 'rap')\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp places365_standard/train/canal-urban/00000559.jpg ./inp.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n",
      "[[162 182 185 ... 130 129 129]\n",
      " [170 202 197 ... 132 130 129]\n",
      " [180 207 208 ... 133 130 130]\n",
      " ...\n",
      " [127 128 128 ... 129 129 129]\n",
      " [127 128 128 ... 129 130 129]\n",
      " [127 127 127 ... 129 129 129]]\n"
     ]
    }
   ],
   "source": [
    "path = 'output_heatmap_lrp.png'\n",
    "with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        print(img.mode)\n",
    "        img.convert('RGB')\n",
    "\n",
    "print(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_ptorch",
   "language": "python",
   "name": "gpu_ptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
